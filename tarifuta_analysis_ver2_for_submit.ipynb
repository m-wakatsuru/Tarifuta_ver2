{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import MeCab\n",
    "mpl.rcParams['font.family'] = \"IPAexGothic\"\n",
    "#scikit learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "#gensim\n",
    "import gensim\n",
    "from gensim import corpora, matutils\n",
    "\n",
    "# Setting 80 characters in a column\n",
    "pd.set_option(\"display.max_colwidth\", 280)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gethering from twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting customer key and access token\n",
    "# Deleted below 4 keys due to confidential information\n",
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    "access_token = ''\n",
    "access_secret = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting api value\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating column list and searched tweet id\n",
    "columns_name=[\"TW_NO\",\"TW_TIME\",\"TW_TEXT\",\"FAV\",\"RT\"]\n",
    "tw_id ='tarinaifutari'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrame\n",
    "tweet_data = []\n",
    "for tweet in tweepy.Cursor(api.user_timeline,screen_name = tw_id,exclude_replies = True).items():\n",
    "    tweet_data.append([tweet.id,tweet.created_at+datetime.timedelta(hours=9),tweet.text.replace('\\n',''),tweet.favorite_count,tweet.retweet_count])\n",
    "df = pd.DataFrame(tweet_data,columns=columns_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TW_NO</th>\n",
       "      <th>TW_TIME</th>\n",
       "      <th>TW_TEXT</th>\n",
       "      <th>FAV</th>\n",
       "      <th>RT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1211708062337126400</td>\n",
       "      <td>2019-12-31 02:58:09</td>\n",
       "      <td>…文面から棒読みが伝わりますが、なにせ第2章が始まったばかりなのでご勘弁ください。さよなら たりないふたりこんにちは たりないふたり文責 山里 https://t.co/T4WcRLwvOh</td>\n",
       "      <td>3063</td>\n",
       "      <td>447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1211707984801189896</td>\n",
       "      <td>2019-12-31 02:57:50</td>\n",
       "      <td>放送ご覧頂きありがとうございました。改めまして皆様、ご指導、ご鞭撻の程よろしくお願いします。文責 若林</td>\n",
       "      <td>3809</td>\n",
       "      <td>520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1211707917176426497</td>\n",
       "      <td>2019-12-31 02:57:34</td>\n",
       "      <td>放送ご覧頂きありがとうございました。お知らせです。Huluで完全版の配信がスタートしました。色んな理由でカットされた部分や舞台裏映像も入って3時間位あるそうです。ライブグッズの通販も始まったのでぜひ！文責 山里</td>\n",
       "      <td>2989</td>\n",
       "      <td>484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1209752987540705280</td>\n",
       "      <td>2019-12-25 17:29:23</td>\n",
       "      <td>どうかしてるよ…それ、大人の都合で絶対まだ言ってはいけないって説明されたでしょ？文責　山里</td>\n",
       "      <td>2779</td>\n",
       "      <td>426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1209724189575282688</td>\n",
       "      <td>2019-12-25 15:34:57</td>\n",
       "      <td>さよならたりないふたり完全版はHuluで放送直後から配信。テレビ版でカットしたところや舞台裏ドキュメント入って3時間弱です。←この情報だけゲットしました。最近、解禁前の情報を横流しできずすみません。　文責　若林</td>\n",
       "      <td>6397</td>\n",
       "      <td>1523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 TW_NO             TW_TIME  \\\n",
       "0  1211708062337126400 2019-12-31 02:58:09   \n",
       "1  1211707984801189896 2019-12-31 02:57:50   \n",
       "2  1211707917176426497 2019-12-31 02:57:34   \n",
       "3  1209752987540705280 2019-12-25 17:29:23   \n",
       "4  1209724189575282688 2019-12-25 15:34:57   \n",
       "\n",
       "                                                                                                     TW_TEXT  \\\n",
       "0            …文面から棒読みが伝わりますが、なにせ第2章が始まったばかりなのでご勘弁ください。さよなら たりないふたりこんにちは たりないふたり文責 山里 https://t.co/T4WcRLwvOh   \n",
       "1                                                        放送ご覧頂きありがとうございました。改めまして皆様、ご指導、ご鞭撻の程よろしくお願いします。文責 若林   \n",
       "2  放送ご覧頂きありがとうございました。お知らせです。Huluで完全版の配信がスタートしました。色んな理由でカットされた部分や舞台裏映像も入って3時間位あるそうです。ライブグッズの通販も始まったのでぜひ！文責 山里   \n",
       "3                                                              どうかしてるよ…それ、大人の都合で絶対まだ言ってはいけないって説明されたでしょ？文責　山里   \n",
       "4  さよならたりないふたり完全版はHuluで放送直後から配信。テレビ版でカットしたところや舞台裏ドキュメント入って3時間弱です。←この情報だけゲットしました。最近、解禁前の情報を横流しできずすみません。　文責　若林   \n",
       "\n",
       "    FAV    RT  \n",
       "0  3063   447  \n",
       "1  3809   520  \n",
       "2  2989   484  \n",
       "3  2779   426  \n",
       "4  6397  1523  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangling data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting the space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the space in twitter text\n",
    "df['TW_TEXT'] = df['TW_TEXT'].apply(lambda x: re.sub('([あ-んア-ン一-龥ー])\\s+((?=[あ-んア-ン一-龥ー]))',r'\\1\\2', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new column \"person\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create person column to clarify who tweeted.\n",
    "# Firstly, I filtered the word \"文責山里\" and \"文責若林\" following their rule\n",
    "df['PERSON'] ='不明'\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    if '文責若林' in df.loc[i,'TW_TEXT']:\n",
    "        df.loc[i, 'PERSON']='若林'\n",
    "    elif '文責山里' in df.loc[i,'TW_TEXT'] :\n",
    "        df.loc[i, 'PERSON']='山里'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifying the filter\n",
    "# ['文責山里若林', '文責山里・若林'] → 双方\n",
    "for b_word in ['文責山里若林', '文責山里・若林']:\n",
    "    for i in range(df.shape[0]):\n",
    "        if b_word in df.loc[i,'TW_TEXT']:\n",
    "            df.loc[i,'PERSON']='双方'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if the above change is working or not\n",
    "df[df['PERSON']=='双方']['TW_TEXT'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "537"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the tweet not following the rule\n",
    "df[df['PERSON']=='不明']['TW_TEXT'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5                                                                                                            さよなら？いや、こんにちは。 https://t.co/kb0drHfsr9\n",
       "40         RT @thetvjp: 「たりないふたり」5年ぶり復活で“さよなら”のワケ企画演出・安島隆氏が語る山里亮太×若林正恭の10年 #たりないふたり #たりふた #ライブ #山里亮太 #若林正恭 #安島隆 @tarinaifutari @takashiajimahttps:…\n",
       "82     たりないふたりファンの方に現状報告です。2人で会える日がなく山ちゃんはスッキリの見守り終わり、ぼくは日テレのレギュラー番組の前に安島さんが楽屋にやって来て「ハロウィンなんかは今どう思う？」と聞かれてはそれを伝言しに行くというスタイ… https://t.co/rPpQ3HRBVm\n",
       "165                                                                                                                  チュウ、チュウ、次がラストの収録。色々と企みチュウ。文責悪鼠\n",
       "167                                                                                                                              チュウ、チュウ、ロケチュウ。文責悪鼠\n",
       "Name: TW_TEXT, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the tweet not following the rule\n",
    "df[df['PERSON']=='不明']['TW_TEXT'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the other word which we can filter the person by.\n",
    "# ['文責悪鼠', '歌責若林', 'MASA', '文責・若林', '文責若']　→ 若林\n",
    "# ['山里亮太'] → 山里\n",
    "\n",
    "for w_word in ['文責悪鼠', '歌責若林', 'MASA', '文責・若林', '文責若']:\n",
    "    for i in range(df.shape[0]):\n",
    "        if df.loc[i,'PERSON']=='不明':\n",
    "            if w_word in df.loc[i,'TW_TEXT']:\n",
    "                df.loc[i,'PERSON']='若林'\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    if df.loc[i,'PERSON']=='不明':\n",
    "        if '山里亮太' in df.loc[i,'TW_TEXT']:\n",
    "            df.loc[i,'PERSON']='山里'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "511"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the tweet not following the rule\n",
    "df[df['PERSON']=='不明']['TW_TEXT'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[(df['PERSON']=='山里') | (df['PERSON']=='若林')].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Person Label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PERSON_LABEL']=''\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    if df.loc[i, 'PERSON'] =='若林':\n",
    "        df.loc[i, 'PERSON_LABEL'] =1\n",
    "    elif df.loc[i, 'PERSON'] =='山里':\n",
    "        df.loc[i, 'PERSON_LABEL'] =0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing data into csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('tarifuta_analysis_ver2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tarifuta_analysis_ver2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>TW_NO</th>\n",
       "      <th>TW_TIME</th>\n",
       "      <th>TW_TEXT</th>\n",
       "      <th>FAV</th>\n",
       "      <th>RT</th>\n",
       "      <th>PERSON</th>\n",
       "      <th>PERSON_LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1211708062337126400</td>\n",
       "      <td>2019-12-31 02:58:09</td>\n",
       "      <td>…文面から棒読みが伝わりますが、なにせ第2章が始まったばかりなのでご勘弁ください。さよならたりないふたりこんにちはたりないふたり文責山里 https://t.co/T4WcRLwvOh</td>\n",
       "      <td>3063</td>\n",
       "      <td>447</td>\n",
       "      <td>山里</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1211707984801189896</td>\n",
       "      <td>2019-12-31 02:57:50</td>\n",
       "      <td>放送ご覧頂きありがとうございました。改めまして皆様、ご指導、ご鞭撻の程よろしくお願いします。文責若林</td>\n",
       "      <td>3809</td>\n",
       "      <td>520</td>\n",
       "      <td>若林</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1211707917176426497</td>\n",
       "      <td>2019-12-31 02:57:34</td>\n",
       "      <td>放送ご覧頂きありがとうございました。お知らせです。Huluで完全版の配信がスタートしました。色んな理由でカットされた部分や舞台裏映像も入って3時間位あるそうです。ライブグッズの通販も始まったのでぜひ！文責山里</td>\n",
       "      <td>2989</td>\n",
       "      <td>484</td>\n",
       "      <td>山里</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                TW_NO              TW_TIME  \\\n",
       "0      0  1211708062337126400  2019-12-31 02:58:09   \n",
       "1      1  1211707984801189896  2019-12-31 02:57:50   \n",
       "2      2  1211707917176426497  2019-12-31 02:57:34   \n",
       "\n",
       "                                                                                                    TW_TEXT  \\\n",
       "0              …文面から棒読みが伝わりますが、なにせ第2章が始まったばかりなのでご勘弁ください。さよならたりないふたりこんにちはたりないふたり文責山里 https://t.co/T4WcRLwvOh   \n",
       "1                                                        放送ご覧頂きありがとうございました。改めまして皆様、ご指導、ご鞭撻の程よろしくお願いします。文責若林   \n",
       "2  放送ご覧頂きありがとうございました。お知らせです。Huluで完全版の配信がスタートしました。色んな理由でカットされた部分や舞台裏映像も入って3時間位あるそうです。ライブグッズの通販も始まったのでぜひ！文責山里   \n",
       "\n",
       "    FAV   RT PERSON  PERSON_LABEL  \n",
       "0  3063  447     山里             0  \n",
       "1  3809  520     若林             1  \n",
       "2  2989  484     山里             0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization for MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding: utf8\n",
    "from __future__ import unicode_literals\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def unicode_normalize(cls, s):\n",
    "    pt = re.compile('([{}]+)'.format(cls))\n",
    "\n",
    "    def norm(c):\n",
    "        return unicodedata.normalize('NFKC', c) if pt.match(c) else c\n",
    "\n",
    "    s = ''.join(norm(x) for x in re.split(pt, s))\n",
    "    s = re.sub('－', '-', s)\n",
    "    return s\n",
    "\n",
    "def remove_extra_spaces(s):\n",
    "    s = re.sub('[ 　]+', ' ', s)\n",
    "    blocks = ''.join(('\\u4E00-\\u9FFF',  # CJK UNIFIED IDEOGRAPHS\n",
    "                      '\\u3040-\\u309F',  # HIRAGANA\n",
    "                      '\\u30A0-\\u30FF',  # KATAKANA\n",
    "                      '\\u3000-\\u303F',  # CJK SYMBOLS AND PUNCTUATION\n",
    "                      '\\uFF00-\\uFFEF'   # HALFWIDTH AND FULLWIDTH FORMS\n",
    "                      ))\n",
    "    basic_latin = '\\u0000-\\u007F'\n",
    "\n",
    "    def remove_space_between(cls1, cls2, s):\n",
    "        p = re.compile('([{}]) ([{}])'.format(cls1, cls2))\n",
    "        while p.search(s):\n",
    "            s = p.sub(r'\\1\\2', s)\n",
    "        return s\n",
    "\n",
    "    s = remove_space_between(blocks, blocks, s)\n",
    "    s = remove_space_between(blocks, basic_latin, s)\n",
    "    s = remove_space_between(basic_latin, blocks, s)\n",
    "    return s\n",
    "\n",
    "def normalize_neologd(s):\n",
    "    s = s.strip()\n",
    "    s = unicode_normalize('０-９Ａ-Ｚａ-ｚ｡-ﾟ', s)\n",
    "\n",
    "    def maketrans(f, t):\n",
    "        return {ord(x): ord(y) for x, y in zip(f, t)}\n",
    "\n",
    "    s = re.sub('[˗֊‐‑‒–⁃⁻₋−]+', '-', s)  # normalize hyphens\n",
    "    s = re.sub('[﹣－ｰ—―─━ー]+', 'ー', s)  # normalize choonpus\n",
    "    s = re.sub('[~∼∾〜〰～]', '', s)  # remove tildes\n",
    "    s = re.sub('https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-…]+', '', s)  # remove https link\n",
    "    s = re.sub('<[^>]*?>', '', s)  # remove https link\n",
    "    s = s.translate(\n",
    "        maketrans('!\"#$%&\\'()*+,-./:;<=>?@[¥]^_`{|}~｡､･｢｣',\n",
    "              '！”＃＄％＆’（）＊＋，－．／：；＜＝＞？＠［￥］＾＿｀｛｜｝〜。、・「」'))\n",
    "\n",
    "    s = remove_extra_spaces(s)\n",
    "    s = unicode_normalize('！”＃＄％＆’（）＊＋，－．／：；＜＞？＠［￥］＾＿｀｛｜｝〜', s)  # keep ＝,・,「,」\n",
    "    s = re.sub('[’]', '\\'', s)\n",
    "    s = re.sub('[”]', '\"', s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrameの形態要素解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(df,column):\n",
    "    '''\n",
    "    記事群のdictについて、形態素解析してリストにして返す\n",
    "    '''\n",
    "    ret = []\n",
    "    for i in range(df.shape[0]):\n",
    "        ret.append(get_words_main(df.loc[i,column]))\n",
    "    return ret\n",
    "\n",
    "def get_words_main(content):\n",
    "    '''\n",
    "    一つの記事を形態素解析して返す\n",
    "    '''\n",
    "    return [token for token in tokenize(content)]\n",
    "\n",
    "def tokenize(text):\n",
    "    '''\n",
    "    とりあえず形態素解析して全ての品詞を取り出す感じにしてる\n",
    "    '''\n",
    "    node = mecab.parseToNode(text)\n",
    "    while node:\n",
    "        if node.feature.split(',')[0] == \"名詞\":\n",
    "            yield node.surface.lower()\n",
    "        node = node.next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete \"文責\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_bunseki(df,column):\n",
    "    ret =[]\n",
    "    re_person = '(文責(若林|悪鼠|・若林|若|山里|謎のラッパー|たり林てる恭|逃げ林|デロリアン山里))|(歌責若林)|MASA|山里亮太'\n",
    "    for i in range(df.shape[0]):\n",
    "        ret.append(re.sub(re_person,'',df.loc[i,column]))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vectorの取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(dictionary, content):\n",
    "    '''\n",
    "    ある記事の特徴語カウント\n",
    "    '''\n",
    "    tmp = dictionary.doc2bow(get_words_main(content))\n",
    "    dense = list(matutils.corpus2dense([tmp], num_terms=len(dictionary)).T[0])\n",
    "    return dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = MeCab.Tagger ('-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TW_TEXT']=delete_bunseki(df,'TW_TEXT')\n",
    "\n",
    "df['TW_TEXT_VECTOR']=\"\"\n",
    "for i in range(df.shape[0]):\n",
    "    df.loc[i,'TW_TEXT_VECTOR']= normalize_neologd(df.loc[i,'TW_TEXT'])\n",
    "\n",
    "df['TW_TEXT_VECTOR']=get_words(df,'TW_TEXT_VECTOR')\n",
    "\n",
    "dictionary = corpora.Dictionary(df['TW_TEXT_VECTOR'])\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.3)\n",
    "dictionary.save_as_text('tarifuta_dict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary.load_from_text('tarifuta_dict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = []\n",
    "label_train = []\n",
    "for i in range(df.shape[0]):\n",
    "    data_train.append(get_vector(dictionary, df.loc[i,'TW_TEXT']))\n",
    "    label_train.append(df.loc[i,'PERSON_LABEL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== 学習データと予測データが一緒の場合\n",
      "0.9054996127033308\n",
      "==== 学習データと予測データが違う場合\n",
      "0.781733746130031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Waka/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/Waka/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid=[{'n_estimators': [10, 30, 50, 70, 90, 110, 130, 150], 'max_features': ['auto', 'sqrt', 'log2', None]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分類器\n",
    "estimator = RandomForestClassifier()\n",
    "\n",
    "# 学習\n",
    "estimator.fit(data_train, label_train)\n",
    "\n",
    "# 学習したデータを予測にかけてみる（ズルなので正答率高くないとおかしい）\n",
    "print(\"==== 学習データと予測データが一緒の場合\")\n",
    "print(estimator.score(data_train, label_train))\n",
    "\n",
    "# 学習データと試験データに分けてみる\n",
    "data_train_s, data_test_s, label_train_s, label_test_s = train_test_split(data_train, label_train, test_size=0.5)\n",
    "\n",
    "# 分類器をもう一度定義\n",
    "estimator2 = RandomForestClassifier()\n",
    "\n",
    "# 学習\n",
    "estimator2.fit(data_train_s, label_train_s)\n",
    "print(\"==== 学習データと予測データが違う場合\")\n",
    "print(estimator2.score(data_test_s, label_test_s))\n",
    "\n",
    "# グリッドサーチやってみる\n",
    "tuned_parameters = [{'n_estimators': [10, 30, 50, 70, 90, 110, 130, 150], 'max_features': ['auto', 'sqrt', 'log2', None]}]\n",
    "\n",
    "clf = GridSearchCV(RandomForestClassifier(), tuned_parameters, cv=2, scoring='accuracy', n_jobs=-1)\n",
    "clf.fit(data_train_s, label_train_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== グリッドサーチ\n",
      "  ベストパラメタ\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=110, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "  ベストスコア\n",
      "0.751937984496124\n"
     ]
    }
   ],
   "source": [
    "print(\"==== グリッドサーチ\")\n",
    "print(\"  ベストパラメタ\")\n",
    "print(clf.best_estimator_)\n",
    "print(\"  ベストスコア\")\n",
    "print(clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
